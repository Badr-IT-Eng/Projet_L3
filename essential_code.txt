Essential Code Files
==================

File: detection-service/object_detector.py
==================

#!/usr/bin/env python3
"""
RECOVR Lost Object Detector
Detects objects that have been left unattended for too long
"""

import cv2
import torch
import numpy as np
from pathlib import Path
import logging
from datetime import datetime
import json
import requests
from collections import defaultdict
import time
import os
from PIL import Image
import io
import uuid
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), 'yolov5'))
from models.common import DetectMultiBackend
from utils.general import non_max_suppression
import argparse

os.environ['QT_QPA_PLATFORM'] = 'xcb'

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ObjectDetector:
    def __init__(self, model_path, num_classes=29, device=None):
        """Initialize the object detector"""
        self.device = device if device is not None else ('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {self.device}")
        
        # Load model from local yolov5 repo
        if os.path.isabs(model_path) and os.path.exists(model_path):
            weights = model_path
        else:
            # Try current directory first
            local_path = os.path.join(os.path.dirname(__file__), model_path)
            yolov5_path = os.path.join(os.path.dirname(__file__), 'yolov5', model_path)
            if os.path.exists(local_path):
                weights = local_path
            elif os.path.exists(yolov5_path):
                weights = yolov5_path
            else:
                raise FileNotFoundError(f"Model file not found: {model_path}")
        
        # Load model with optimizations
        self.model = DetectMultiBackend(weights, device=self.device)
        self.model.eval()  # Set to evaluation mode
        if self.device == 'cuda':
            self.model = self.model.half()  # Use FP16 for faster inference
        print(f"Model loaded successfully from {weights}")
        
        # Initialize video capture
        self.cap = None
        
        # Initialize tracking
        self.object_positions = {}  # {tracking_id: (position, last_moved_time, reported)}
        self.stationary_threshold = 5.0  # seconds
        
        # Configuration for optimization
        self.target_size = (416, 416)  # Smaller size for faster processing
        self.conf_threshold = 0.05  # VERY LOW: For debugging to see all detections
        self.iou_threshold = 0.45
        
        # Focus on common lost objects
        self.target_classes = {
            'backpack': 'BAG',
            'handbag': 'BAG',
            'suitcase': 'BAG',
            'bottle': 'CONTAINER',
            'laptop': 'ELECTRONICS',
            'cell phone': 'ELECTRONICS'
        }
        
        # Batch processing for better performance
        self.batch_size = 4
        self.frame_buffer = []
        
        # Category mapping for database
        self.category_mapping = {
            'backpack': 'BAGS',
            'handbag': 'BAGS',
            'suitcase': 'BAGS',
            'bottle': 'CONTAINER',
            'cup': 'CONTAINER',
            'wine glass': 'CONTAINER',
            'chair': 'FURNITURE',
            'couch': 'FURNITURE',
            'bed': 'FURNITURE',
            'dining table': 'FURNITURE',
            'toilet': 'FURNITURE',
            'tv': 'ELECTRONICS',
            'mouse': 'ELECTRONICS',
            'remote': 'ELECTRONICS',
            'keyboard': 'ELECTRONICS',
            'microwave': 'APPLIANCE',
            'oven': 'APPLIANCE',
            'toaster': 'APPLIANCE',
            'sink': 'APPLIANCE',
            'refrigerator': 'APPLIANCE',
            'book': 'PAPER',
            'clock': 'DECORATION',
            'vase': 'DECORATION',
            'scissors': 'TOOL',
            'toothbrush': 'PERSONAL',
            'umbrella': 'ACCESSORY',
            'hair drier': 'PERSONAL'
        }
        
        # Configuration pour la détection d'objets stationnaires
        self.position_threshold = 50  # Pixels de déplacement maximum pour considérer un objet comme stationnaire
        self.tracked_objects = defaultdict(lambda: {
            'first_seen': None,
            'last_position': None,
            'last_seen': None,
            'reported': False
        })
        
        # Focus on suitcase detection (retained for drawing, but will process all detections now)
        self.target_class = 'suitcase'

    def start_video(self, video_path):
        """Start video capture from file or camera"""
        if self.cap is not None:
            self.cap.release()
            
        if isinstance(video_path, int):  # Camera index
            self.cap = cv2.VideoCapture(video_path)
        else:  # Video file
            self.cap = cv2.VideoCapture(video_path)
            
        if not self.cap.isOpened():
            raise ValueError(f"Could not open video source: {video_path}")
            
        # Obtenir les dimensions originales
        width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        return width, height
    
    def process_frame(self, frame):
        """Process a single frame and return detections"""
        if frame is None:
            return []
        
        # Resize for faster processing
        frame_resized = cv2.resize(frame, self.target_size)
        
        # Convert to RGB and normalize
        frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
        frame_tensor = torch.from_numpy(frame_rgb).float().div(255.0)
        frame_tensor = frame_tensor.permute(2, 0, 1).unsqueeze(0)
        frame_tensor = frame_tensor.to(self.device)
        
        if self.device == 'cuda':
            frame_tensor = frame_tensor.half()  # Use FP16 for faster inference

        # Inference
        with torch.no_grad():
            pred = self.model(frame_tensor)
            pred = non_max_suppression(pred, conf_thres=self.conf_threshold, iou_thres=self.iou_threshold)[0]

        detections = []
        current_time = datetime.now()
        
        if pred is not None and len(pred):
            for *xyxy, conf, cls in pred.cpu().numpy():
                x1, y1, x2, y2 = xyxy
                conf = float(conf)
                cls = int(cls)
                class_name = self.model.names[cls] if hasattr(self.model, 'names') else str(cls)
                
                # Convert coordinates to original size
                scale_x = frame.shape[1] / self.target_size[0]
                scale_y = frame.shape[0] / self.target_size[1]
                
                x1, x2 = x1 * scale_x, x2 * scale_x
                y1, y2 = y1 * scale_y, y2 * scale_y
                
                # Calculate center
                center_x = (x1 + x2) / 2
                center_y = (y1 + y2) / 2
                
                # Generate unique ID
                obj_id = f"{class_name}_{int(center_x)}_{int(center_y)}"
                
                # Check if stationary (keep this logic for the backend, but show all detections)
                is_stationary = self.is_stationary(obj_id, (center_x, center_y), current_time)
                
                detection = {
                    'class': class_name,
                    'category': self.target_classes.get(class_name, 'MISCELLANEOUS'), # Use get to avoid KeyError
                    'confidence': conf,
                    'bbox': [int(x1), int(y1), int(x2), int(y2)],
                    'tracking_id': str(uuid.uuid4()),
                    'timestamp': current_time.isoformat(),
                    'is_stationary': is_stationary # Include this for debugging
                }
                detections.append(detection)
                print(f"DEBUG: Detected {class_name} with confidence: {conf:.2f} (Stationary: {is_stationary})") # Verbose logging
        
        return detections

    def process_video(self, video_path, output_path=None, save_to_db=True, location=None, skip_frames=4):
        """Process entire video and optionally save results"""
        try:
            width, height = map(int, self.start_video(video_path))
            print(f"Processing video: {video_path}")
            print(f"Resolution: {width}x{height}")
            print(f"Location: {location}")
            
            frame_count = 0
            all_detections = []
            out = None
            
            while True:
                ret, frame = self.cap.read()
                if not ret:
                    break
                    
                frame_count += 1
                if frame_count % skip_frames != 0:
                    continue
                
                # Process frame
                detections = self.process_frame(frame)
                
                if detections:
                    # Draw detections (draws all detections now)
                    frame = self._draw_detections(frame, detections)
                    
                    # Save to database (only if stationary and enabled)
                    if save_to_db:
                        for det in detections:
                            if det.get('is_stationary', False) and det['class'] in self.target_classes: # Only save target stationary classes
                                self._save_to_database(det, frame)
                    
                    all_detections.extend(detections)
                
                # Save output video if requested
                if output_path:
                    if out is None:
                        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                        out = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))
                    out.write(frame)
                
                # Show frame
                cv2.imshow('Detection', frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
            
            # Cleanup
            if out is not None:
                out.release()
            self.cap.release()
            cv2.destroyAllWindows()
            
            return all_detections
            
        except Exception as e:
            logger.error(f"Error processing video: {str(e)}")
            if self.cap is not None:
                self.cap.release()
            if out is not None:
                out.release()
            cv2.destroyAllWindows()
            raise
    
    def _draw_detections(self, frame, detections):
        """Draw detection boxes on frame"""
        for det in detections:
            x1, y1, x2, y2 = det['bbox']
            conf = det['confidence']
            cls_name = det['class'] # Use cls_name from detection dict
            is_stationary = det.get('is_stationary', False)
            
            # Special highlighting for target classes (e.g., suitcases)
            if cls_name == self.target_class:
                color = (0, 255, 255)  # Yellow for target class
                thickness = 3
                label = f"{cls_name}: {conf:.2f}"
                if is_stationary:
                    label += " (STATIONARY)"
                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)
            else:
                color = (0, 255, 0)  # Green for other objects
                thickness = 1
                label = f"{cls_name}: {conf:.2f}"
                if is_stationary:
                    label += " (STATIONARY)"
                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness)
        
        return frame
    
    def _save_detection_image(self, frame, det):
        """Save detection image to file"""
        x1, y1, x2, y2 = det['bbox']
        
        # Create snapshots directory if it doesn't exist
        os.makedirs('snapshots', exist_ok=True)
        
        # Crop the object from the frame
        try:
            object_img = frame[y1:y2, x1:x2]
            if object_img.size == 0:
                print(f"❌ Empty crop from box [{x1}, {y1}, {x2}, {y2}]")
                # Use a small part of the frame instead
                center_x, center_y = frame.shape[1] // 2, frame.shape[0] // 2
                x1, y1 = max(0, center_x - 50), max(0, center_y - 50)
                x2, y2 = min(frame.shape[1]-1, center_x + 50), min(frame.shape[0]-1, center_y + 50)
                object_img = frame[y1:y2, x1:x2]
            
            # Generate a unique filename with timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
            class_name = det['class'].replace(' ', '_')
            filename = f"snapshots/{class_name}_{timestamp}.jpg"
            
            # Save the image
            cv2.imwrite(filename, object_img)
            
            return filename
        except Exception as e:
            print(f"❌ Error saving detection image: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _save_to_database(self, det, frame):
        """Save detections to database via API"""
        api_url = "http://localhost:8082/api/items"

        try:
            # Save the detection image
            image_path = self._save_detection_image(frame, det)
            if not image_path:
                print("❌ Failed to save detection image")
                return False
            
            # Convert relative path to absolute for API
            abs_image_path = os.path.abspath(image_path)
            print(f"📸 Saved detection image to: {abs_image_path}")
            
            # Map COCO class to item category
            category = self.category_mapping.get(det['class'], 'MISCELLANEOUS')
            
            # Calculate coordinates for location description
            x1, y1, x2, y2 = det['bbox']
            center_x = (x1 + x2) / 2
            center_y = (y1 + y2) / 2
            
            # Prepare image data for upload
            try:
                with open(abs_image_path, 'rb') as img_file:
                    img_data = img_file.read()
                print(f"📦 Read image file: {len(img_data)} bytes")
            except Exception as e:
                print(f"❌ Error reading image file: {e}")
                return False
            
            # First, upload the image
            files = {'file': (f"{det['class']}.jpg", img_data, 'image/jpeg')}
            upload_url = "http://localhost:8082/api/files/detection/upload"
            print(f"📤 Uploading image to: {upload_url}")
            
            try:
                upload_response = requests.post(upload_url, files=files)
                print(f"📥 Upload response status: {upload_response.status_code}")
                print(f"📥 Upload response text: {upload_response.text}")
                upload_response.raise_for_status()
            except requests.exceptions.RequestException as e:
                print(f"❌ Error uploading image: {e}")
                if hasattr(e.response, 'text'):
                    print(f"Response text: {e.response.text}")
                return False
            
            # Get the image URL from response
            image_url = upload_response.text.strip('"')
            print(f"🔗 Got image URL: {image_url}")
            
            # Prepare data for API
            data = {
                'name': f"Detected {det['class']}",
                'description': f"Automatically detected {det['class']} with confidence {det['confidence']:.2f}",
                'category': category,
                'status': "LOST",
                'type': "LOST",
                'location': f"Camera Feed (X:{center_x:.1f}, Y:{center_y:.1f})",
                'imageUrl': image_url,
                'reportedAt': det['timestamp']
            }

            headers = {'Content-Type': 'application/json'}
            print(f"📤 Creating item with data: {data}")

            # Create item in database
            try:
                response = requests.post(api_url, json=data, headers=headers)
                print(f"📥 Create item response status: {response.status_code}")
                print(f"📥 Create item response text: {response.text}")
                
                if response.status_code in (200, 201):
                    print(f"✅ Saved detection to database: {data['name']}")
                    return True
                else:
                    print(f"❌ Failed to save detection: {response.status_code}")
                    print(f"Response: {response.text}")
                    return False
            except requests.exceptions.RequestException as e:
                print(f"❌ Error creating item: {e}")
                if hasattr(e.response, 'text'):
                    print(f"Response text: {e.response.text}")
                return False
        
        except Exception as e:
            print(f"❌ Error saving to database: {e}")
            import traceback
            traceback.print_exc()
            return False

    def get_class_names(self):
        """Get list of class names"""
        return [
            'background',
            'suitcase', 'backpack', 'bag', 'handbag', 'briefcase',  # Luggage items
            'person', 'human',  # People
            'chair', 'bench', 'seat',  # Furniture
            'table', 'desk',  # Surfaces
            'door', 'window',  # Building elements
            'wall', 'floor',  # Structures
            'light', 'lamp',  # Lighting
            'plant', 'tree',  # Nature
            'car', 'vehicle',  # Vehicles
            'bicycle', 'motorcycle',  # Two-wheelers
            'trash', 'bin',  # Waste
            'sign', 'poster',  # Information
            'clock', 'watch',  # Time
            'phone', 'laptop',  # Electronics
            'book', 'newspaper',  # Reading material
            'food', 'drink'  # Consumables
        ]

    def is_stationary(self, obj_id, current_pos, current_time):
        """Check if an object has been stationary for too long"""
        if obj_id not in self.object_positions:
            self.object_positions[obj_id] = (current_pos, current_time, False)
            return False
            
        last_pos, last_time, reported = self.object_positions[obj_id]
        
        # Check if object has moved significantly
        if np.linalg.norm(np.array(current_pos) - np.array(last_pos)) > 20:  # moved > 20px
            self.object_positions[obj_id] = (current_pos, current_time, False)
            return False
            
        # Check if object has been stationary for too long
        time_stationary = current_time - last_time
        if time_stationary.total_seconds() >= self.stationary_threshold and not reported:
            self.object_positions[obj_id] = (last_pos, last_time, True)
            return True
            
        return False

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Lost Object Detection Service')
    parser.add_argument('--model', required=True, help='Path to the YOLOv5 model file')
    parser.add_argument('--video', help='Path to video file (optional)')
    parser.add_argument('--output', help='Path to save output video (optional)')
    parser.add_argument('--no-backend', action='store_true', help='Disable backend integration')
    parser.add_argument('--camera-id', type=int, help='Camera ID for live detection')
    parser.add_argument('--camera-location', help='Location description for camera feed')
    
    args = parser.parse_args()
    
    # Initialize detector
    detector = ObjectDetector(args.model, num_classes=29)
    
    # Process video or camera feed
    if args.camera_id is not None:
        detector.process_video(
            args.camera_id,
            output_path=args.output,
            save_to_db=not args.no_backend,
            location=args.camera_location
        )
    elif args.video:
        detector.process_video(
            args.video,
            output_path=args.output,
            save_to_db=not args.no_backend
        )
    else:
        print("Please provide either --video or --camera-id") 
----------------------------------------

File: detection-service/run_detection.py
==================

#!/usr/bin/env python3
"""
Script pour exécuter la détection d'objets sur une image individuelle
"""

import argparse
import os
import sys
import json
import cv2
import torch
import logging
from pathlib import Path
from datetime import datetime
from object_detector import ObjectDetector

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def find_latest_model():
    """Trouver le dernier modèle entraîné dans le répertoire"""
    model_files = list(Path('.').glob('stable_model_epoch_*.pth')) + list(Path('.').glob('*.pt'))
    if not model_files:
        logger.error("Aucun fichier modèle trouvé! Placez votre fichier modèle dans le répertoire.")
        return None
    return max(model_files, key=lambda p: p.stat().st_mtime)

def run_detection(image_path, model_path=None, confidence_threshold=0.5, output_path=None):
    """Exécuter la détection sur l'image spécifiée"""
    
    # Vérifier si l'image existe
    if not os.path.exists(image_path):
        logger.error(f"Image non trouvée: {image_path}")
        return []
    
    # Trouver le modèle si non spécifié
    if not model_path:
        model_path = find_latest_model()
        if not model_path:
            return []
    
    logger.info(f"Utilisation du modèle: {model_path}")
    logger.info(f"Traitement de l'image: {image_path}")
    
    try:
        # Charger l'image
        image = cv2.imread(str(image_path))
        if image is None:
            logger.error(f"Impossible de lire l'image: {image_path}")
            return []
        
        # Initialiser le détecteur d'objets
        detector = ObjectDetector(str(model_path))
        
        # Exécuter la détection sur l'image
        detections = detector.process_frame(image)
        
        # Dessiner les boîtes englobantes sur l'image si un chemin de sortie est spécifié
        if output_path:
            output_image = detector._draw_detections(image.copy(), detections)
            cv2.imwrite(output_path, output_image)
            logger.info(f"Image avec détections enregistrée: {output_path}")
        
        # Afficher et retourner les détections
        logger.info(f"Détections trouvées: {len(detections)}")
        for i, det in enumerate(detections):
            logger.info(f"  {i+1}. {det['class_name']} ({det['score']:.2f})")
        
        # Imprimer les détections au format JSON pour faciliter l'analyse par le frontend
        print(f"DETECTIONS: {json.dumps(detections)}")
        
        return detections
        
    except Exception as e:
        logger.error(f"Erreur lors de la détection: {e}")
        import traceback
        traceback.print_exc()
        return []

def main():
    parser = argparse.ArgumentParser(description='Détecter des objets dans une image')
    parser.add_argument('--image', required=True, help='Chemin vers l\'image à analyser')
    parser.add_argument('--model', help='Chemin vers le fichier du modèle (optionnel)')
    parser.add_argument('--threshold', type=float, default=0.5, help='Seuil de confiance pour la détection')
    parser.add_argument('--output', help='Chemin pour enregistrer l\'image avec les détections (optionnel)')
    
    args = parser.parse_args()
    
    run_detection(args.image, args.model, args.threshold, args.output)

if __name__ == "__main__":
    main() 
----------------------------------------

File: detection-service/start_detection.py
==================

#!/usr/bin/env python3
"""
RECOVR Detection Service Starter
Starts the object detection service with proper configuration
"""

import os
import sys
import logging
from pathlib import Path
import argparse
from datetime import datetime
import cv2
from object_detector import ObjectDetector
import time

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(f'detection_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    ]
)

logger = logging.getLogger(__name__)

def main():
    # Configuration du logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    # Configuration des arguments
    parser = argparse.ArgumentParser(description='Object Detection Service')
    parser.add_argument('--video', type=str, help='Path to video file')
    parser.add_argument('--camera', type=int, default=None, help='Camera index (default: None)')
    parser.add_argument('--location', type=str, required=True, help='Location where the detection is happening')
    parser.add_argument('--model', type=str, default='stable_model_epoch_30.pt', help='Path to model file')
    parser.add_argument('--output', type=str, help='Path to save output video')
    parser.add_argument('--skip-frames', type=int, default=2, help='Number of frames to skip between detections')
    args = parser.parse_args()

    try:
        # Initialiser le détecteur
        detector = ObjectDetector(
            model_path=args.model,
            num_classes=29,
            device=None  # Utilisera automatiquement CUDA si disponible
        )

        # Déterminer la source vidéo
        video_source = args.camera if args.camera is not None else args.video
        if video_source is None:
            raise ValueError("Either --video or --camera must be specified")

        logging.info(f"Starting detection from {'camera ' + str(args.camera) if args.camera is not None else 'video file: ' + args.video}")
        logging.info(f"Location: {args.location}")
        logging.info(f"Using model: {args.model}")
        logging.info(f"Skipping {args.skip_frames} frames between detections")

        # Démarrer la détection
        detector.process_video(
            video_path=video_source,
            output_path=args.output,
            save_to_db=True,
            location=args.location,
            skip_frames=args.skip_frames
        )

    except Exception as e:
        logging.error(f"Error in detection service: {str(e)}")
        raise
    finally:
        logging.info("Detection service stopped")

if __name__ == "__main__":
    main() 
----------------------------------------

File: detection-service/test_integration.py
==================

#!/usr/bin/env python3
"""
Test script to verify integration between object detection and Spring Boot backend
"""

import os
import sys
from pathlib import Path
import logging
from datetime import datetime
from object_detector import ObjectDetector
import glob
import requests

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_jwt_token():
    url = "http://localhost:8082/api/auth/signin"
    data = {"username": "admin", "password": "admin"}
    try:
        r = requests.post(url, json=data)
        r.raise_for_status()
        return r.json()["token"]
    except Exception as e:
        print(f"Erreur d'authentification: {e}")
        return None

def main():
    # Chemin de la vidéo à tester
    video_path = "/home/badr/lost-found-system-new/lost-found-system/20250615_034055.mp4"
    if not os.path.exists(video_path):
        logger.error(f"Test video not found: {video_path}")
        return

    # Cherche tous les modèles .pth et .pt dans le dossier courant
    model_paths = sorted(glob.glob("*.pth") + glob.glob("*.pt"))
    if not model_paths:
        logger.error("Aucun modèle .pth ou .pt trouvé dans le dossier.")
        return

    logger.info("=== RECOVR System Integration Test (All Models) ===")
    logger.info(f"Using video: {video_path}")
    logger.info("Ce test va :")
    logger.info("1. Tester chaque modèle sur la vidéo")
    logger.info("2. Envoyer les objets détectés au backend Spring Boot")
    logger.info("3. Afficher les résultats pour chaque modèle")

    # Get JWT token for authenticated requests
    token = get_jwt_token()
    headers = {"Authorization": f"Bearer {token}"} if token else {}

    for model_path in model_paths:
        logger.info(f"\n--- Test du modèle : {model_path} ---")
        try:
            detector = ObjectDetector(model_path)
            os.makedirs('snapshots', exist_ok=True)
            detections = detector.process_video(video_path, output_path=f"output_{model_path}.mp4", save_to_db=True, headers=headers)
            logger.info(f"✅ Modèle {model_path} : {len(detections)} objets détectés et envoyés au backend.")
        except Exception as e:
            logger.error(f"❌ Erreur avec le modèle {model_path} : {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main() 
----------------------------------------

File: detection-service/requirements.txt
==================

opencv-python>=4.8.0
torch>=2.0.0
torchvision>=0.15.0
numpy>=1.24.0
requests>=2.31.0
python-dotenv>=1.0.0
Pillow>=10.0.0
ultralytics>=8.0.0  # For YOLO models
supervision>=0.3.0  # For object tracking
python-multipart>=0.0.6  # Pour l'upload de fichiers
tqdm>=4.65.0  # Pour les barres de progression 
----------------------------------------

File: spring-backend/src/main/resources/application.properties
==================

# Server Configuration
server.port=8082

# Database Configuration (MySQL)
spring.datasource.url=jdbc:mysql://localhost:3306/recovr_db?createDatabaseIfNotExist=true&useSSL=false&serverTimezone=UTC&allowPublicKeyRetrieval=true
spring.datasource.username=recovr_user
spring.datasource.password=Recovr@2024
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver

# JPA Configuration
spring.jpa.database-platform=org.hibernate.dialect.MySQL8Dialect
spring.jpa.hibernate.ddl-auto=validate
spring.jpa.show-sql=true
spring.jpa.properties.hibernate.format_sql=true
spring.jpa.properties.hibernate.jdbc.batch_size=50
spring.jpa.properties.hibernate.order_inserts=true
spring.jpa.properties.hibernate.order_updates=true
spring.jpa.properties.hibernate.batch_versioned_data=true

# Flyway Configuration
spring.flyway.enabled=true
spring.flyway.baseline-on-migrate=true
spring.flyway.locations=classpath:db/migration
spring.flyway.clean-disabled=false

# JWT Configuration
jwt.secret=recovrSecretKey123456789RecovRSecretKey123456789RecovRSecretKey123456789
jwt.expiration=86400000

# File Upload Configuration
spring.servlet.multipart.max-file-size=10MB
spring.servlet.multipart.max-request-size=10MB
file.upload-dir=./uploads

# Logging Configuration
logging.level.root=INFO
logging.level.org.springframework=INFO
logging.level.org.hibernate=INFO
logging.level.com.recovr=DEBUG
logging.level.org.springframework.security=INFO
logging.pattern.console=%d{yyyy-MM-dd HH:mm:ss} - %msg%n

# CORS Configuration
cors.allowed-origins=http://localhost:8082,http://localhost:3000,http://localhost:3001
cors.allowed-methods=GET,POST,PUT,DELETE,OPTIONS
cors.allowed-headers=Authorization,Content-Type,X-Requested-With,Accept,Origin,Access-Control-Request-Method,Access-Control-Request-Headers
cors.exposed-headers=Authorization,Content-Type
cors.max-age=3600

# Application Specific Configuration
app.name=RecovR Lost and Found
app.version=1.0.0
app.description=A comprehensive lost and found management system
----------------------------------------

File: spring-backend/pom.xml
==================

<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>3.2.3</version>
        <relativePath/>
    </parent>

    <groupId>com.recovr</groupId>
    <artifactId>api</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <name>recovr-api</name>
    <description>Backend API for Recovr lost and found system</description>

    <properties>
        <java.version>17</java.version>
        <jjwt.version>0.11.5</jjwt.version>
    </properties>

    <dependencies>
        <!-- Spring Boot Starters -->
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-security</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-validation</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-jpa</artifactId>
        </dependency>

        <!-- Database -->
        <dependency>
            <groupId>com.mysql</groupId>
            <artifactId>mysql-connector-j</artifactId>
            <version>8.0.33</version>
        </dependency>
        <dependency>
            <groupId>org.flywaydb</groupId>
            <artifactId>flyway-core</artifactId>
        </dependency>
        <dependency>
            <groupId>org.flywaydb</groupId>
            <artifactId>flyway-mysql</artifactId>
        </dependency>

        <!-- OpenAPI/Swagger -->
        <dependency>
            <groupId>org.springdoc</groupId>
            <artifactId>springdoc-openapi-starter-webmvc-ui</artifactId>
            <version>2.3.0</version>
        </dependency>
        <dependency>
            <groupId>org.springdoc</groupId>
            <artifactId>springdoc-openapi-starter-common</artifactId>
            <version>2.3.0</version>
        </dependency>
        <dependency>
            <groupId>io.swagger.core.v3</groupId>
            <artifactId>swagger-annotations</artifactId>
            <version>2.2.20</version>
        </dependency>
        <dependency>
            <groupId>io.swagger.core.v3</groupId>
            <artifactId>swagger-models</artifactId>
            <version>2.2.20</version>
        </dependency>

        <!-- JWT -->
        <dependency>
            <groupId>io.jsonwebtoken</groupId>
            <artifactId>jjwt-api</artifactId>
            <version>${jjwt.version}</version>
        </dependency>
        <dependency>
            <groupId>io.jsonwebtoken</groupId>
            <artifactId>jjwt-impl</artifactId>
            <version>${jjwt.version}</version>
            <scope>runtime</scope>
        </dependency>
        <dependency>
            <groupId>io.jsonwebtoken</groupId>
            <artifactId>jjwt-jackson</artifactId>
            <version>${jjwt.version}</version>
            <scope>runtime</scope>
        </dependency>

        <!-- Swagger/OpenAPI -->
        <dependency>
            <groupId>org.springdoc</groupId>
            <artifactId>springdoc-openapi-starter-webmvc-api</artifactId>
            <version>2.3.0</version>
        </dependency>

        <!-- Lombok -->
        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <optional>true</optional>
        </dependency>

        <!-- Test -->
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework.security</groupId>
            <artifactId>spring-security-test</artifactId>
            <scope>test</scope>
        </dependency>

        <!-- Jakarta Annotation -->
        <dependency>
            <groupId>jakarta.annotation</groupId>
            <artifactId>jakarta.annotation-api</artifactId>
            <version>2.1.1</version>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.flywaydb</groupId>
                <artifactId>flyway-maven-plugin</artifactId>
                <version>9.22.3</version>
                <configuration>
                    <url>jdbc:mysql://localhost:3306/recovr_db?useSSL=false&amp;serverTimezone=UTC&amp;allowPublicKeyRetrieval=true</url>
                    <user>recovr_user</user>
                    <password>Recovr@2024</password>
                    <locations>
                        <location>classpath:db/migration</location>
                    </locations>
                    <baselineOnMigrate>true</baselineOnMigrate>
                    <baselineVersion>0</baselineVersion>
                    <cleanDisabled>false</cleanDisabled>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
                <configuration>
                    <excludes>
                        <exclude>
                            <groupId>org.projectlombok</groupId>
                            <artifactId>lombok</artifactId>
                        </exclude>
                    </excludes>
                </configuration>
            </plugin>
        </plugins>
    </build>
</project> 
----------------------------------------

